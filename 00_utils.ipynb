{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bace50d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf01837",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!sh setup.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14526c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "# import json\n",
    "# import re\n",
    "\n",
    "\n",
    "# alphabets= \"([A-Za-z])\"\n",
    "# prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "# suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "# starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "# acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "# websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "# def split_into_sentences(text):\n",
    "#     text = \" \" + text + \"  \"\n",
    "#     text = text.replace(\"\\n\",\" \")\n",
    "#     text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "#     text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "#     if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "#     text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "#     text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "#     text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "#     text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "#     text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "#     text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "#     text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "#     if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "#     if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "#     if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "#     if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "#     text = text.replace(\".\",\".<stop>\")\n",
    "#     text = text.replace(\"?\",\"?<stop>\")\n",
    "#     text = text.replace(\"!\",\"!<stop>\")\n",
    "#     text = text.replace(\"<prd>\",\".\")\n",
    "#     sentences = text.split(\"<stop>\")\n",
    "#     sentences = sentences[:-1]\n",
    "#     sentences = [s.strip() for s in sentences]\n",
    "#     return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ea51cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: biopython in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (1.79)\n",
      "Requirement already satisfied: pydash in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (5.1.0)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from biopython) (1.21.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install biopython pydash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2989197b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "from Bio import Entrez\n",
    "from pydash import get\n",
    "\n",
    "def get_pubmed_records(pmids):\n",
    "    Entrez.email = \"strasser.ms@gmail.com\"\n",
    "    # handle type is http.client.HTTPResponse\n",
    "    handle = Entrez.efetch(\n",
    "        db=\"pubmed\",\n",
    "        id=\",\".join(pmids),\n",
    "        #api_key=\"be67e0a1be023d17fff5334a6c6f45287a08\",\n",
    "        rettype=\"xml\",\n",
    "        retmode=\"text\",\n",
    "    )\n",
    "    records = Entrez.read(handle)\n",
    "    return records\n",
    "\n",
    "def get_attribute_text(AbstractText, NlmCategory):\n",
    "    if AbstractText is None:\n",
    "        return None\n",
    "    for string_element in AbstractText:\n",
    "        cat = \"\"\n",
    "        try:\n",
    "            #the worst API arbitrary nonsense ... 1h of my life for getting data out of it\n",
    "            cat = string_element.attributes[\"NlmCategory\"]\n",
    "        except:\n",
    "            None\n",
    "        \n",
    "        if cat == NlmCategory:\n",
    "            return string_element\n",
    "\n",
    "def get_segment(record, segment_label):\n",
    "    if record is None:\n",
    "        return None\n",
    "    return str(\n",
    "        get_attribute_text(\n",
    "            get(record, \"MedlineCitation.Article.Abstract.AbstractText\"), \n",
    "            segment_label\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8de561",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import itertools\n",
    "from pydash import find_index\n",
    "#OLD one-off functions that need refactoring\n",
    "def sentence_has_phrase(sentence, cue_phrases):\n",
    "    for c in cue_phrases: #unsafe scope\n",
    "        if c in sentence:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "def find_sentence_in_abstract(paragraph, bias=0):\n",
    "    \"\"\"checks paragraph for key sentences. Returns first matching hit\"\"\"\n",
    "    p_sents = split_into_sentences(paragraph)\n",
    "    idx = find_index(p_sents, sentence_has_phrase)\n",
    "    if idx < 0:\n",
    "        return \"\"\n",
    "    \n",
    "    i = idx + bias #bias- for sentence before or after\n",
    "    if i < 0:\n",
    "        return \"\"\n",
    "    return get(p_sents, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f710087e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "\n",
    "# url = \"https://github.com/derekchuank/high-frequency-vocabulary/blob/master/30k.txt\"\n",
    "# res = req.get(url)\n",
    "# vocab30k = res.text.split(\"\\t\\n\")\n",
    "# vocab30k[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8544d112",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import spacy\n",
    "import en_core_web_md\n",
    "with open(\"vocab30k.txt\") as f:\n",
    "    vocab30k = f.read().split(\"\\t\\n\")\n",
    "\n",
    "nlp = en_core_web_md.load()\n",
    "\n",
    "extra_vocab = [\n",
    "               \"-PRON-\", \"-pron-\", \".\", \",\", \";\"  #lemmatizer inconsitency\n",
    "]\n",
    "\n",
    "special_tokens = {\n",
    "    \"(\": \"-lrb-\",\n",
    "    \")\": \"-rrb-\"\n",
    "}\n",
    "\n",
    "def replace_outof_vocab_words(text, vocab, nlp=nlp, extra_vocab=extra_vocab, special_tokens=special_tokens):\n",
    "    vocab += extra_vocab\n",
    "    newtext = \"\"\n",
    "    for token in nlp(text):\n",
    "        t = token.text\n",
    "        if special_tokens.get(t):\n",
    "            t=special_tokens.get(t)\n",
    "        elif token.lemma_.lower() not in vocab and token.tag_ is not None:\n",
    "            t= \"ii\" + token.tag_.lower()\n",
    "        newtext += t + \" \"\n",
    "\n",
    "    return newtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75eeaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace_outof_vocab_words('Pancreaticobiliary maljunction (PBM) refers to the union of the pancreatic and biliary ducts outside of the duodenal wall.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689b7b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLTK\n",
    "patterns = [\n",
    "    (r'.*ing$', 'VBG'),               # gerunds\n",
    "    (r'.*ed$', 'VBD'),                # simple past\n",
    "    (r'.*es$', 'VBZ'),                # 3rd singular present\n",
    "    (r'.*ould$', 'MD'),               # modals\n",
    "    (r'.*\\'s$', 'NN$'),               # possessive nouns\n",
    "    (r'.*s$', 'NNS'),                 # plural nouns\n",
    "    (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),  # cardinal numbers\n",
    "    (r'.*', 'NN')                     # nouns (default)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5621442e",
   "metadata": {},
   "source": [
    "## Text Features with spacy extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5493d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "# nlp.add_pipe('spacytextblob')\n",
    "# text = 'I had a really horrible day. It was the worst day ever! But every now and then I have a really good day that makes me happy.'\n",
    "# doc = nlp(text)\n",
    "# doc._.polarity      # Polarity: -0.125\n",
    "# doc._.subjectivity  # Sujectivity: 0.9\n",
    "# doc._.assessments   # Assessments: [(['really', 'horrible'], -1.0, 1.0, None), (['worst', '!'], -1.0, 1.0, None), (['really', 'good'], 0.7, 0.6000000000000001, None), (['happy'], 0.8, 1.0, None)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87a0ada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9b16b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a6f6d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
