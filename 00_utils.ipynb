{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a951256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44599a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def find_sentence_in_abstract(paragraph, cue_phrases):\n",
    "    \"\"\"checks paragraph for key sentences. Returns first matching hit\"\"\"\n",
    "    p_sents = paragraph.split(\".\")\n",
    "    combinations = itertools.product(p_sents, cue_phrases)\n",
    "    hit = \"\"\n",
    "    for c in combinations:\n",
    "        if c[1] in c[0]:\n",
    "       # if c[0].startswith(c[1]): #easier to work with later assuming beginning of sentence\n",
    "            hit = c[0]\n",
    "            break\n",
    "    return hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f79c1c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9f0ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "from Bio import Entrez\n",
    "from pydash import get\n",
    "\n",
    "def get_pubmed_records(pmids):\n",
    "    Entrez.email = \"strasser.ms@gmail.com\"\n",
    "    # handle type is http.client.HTTPResponse\n",
    "    handle = Entrez.efetch(\n",
    "        db=\"pubmed\",\n",
    "        id=\",\".join(pmids),\n",
    "        #api_key=\"be67e0a1be023d17fff5334a6c6f45287a08\",\n",
    "        rettype=\"xml\",\n",
    "        retmode=\"text\",\n",
    "    )\n",
    "    records = Entrez.read(handle)\n",
    "    return records\n",
    "\n",
    "def get_attribute_text(AbstractText, NlmCategory):\n",
    "    if AbstractText is None:\n",
    "        return None\n",
    "    for string_element in AbstractText:\n",
    "        cat = \"\"\n",
    "        try:\n",
    "            #the worst API arbitrary nonsense ... 1h of my life for getting data out of it\n",
    "            cat = string_element.attributes[\"NlmCategory\"]\n",
    "        except:\n",
    "            None\n",
    "        \n",
    "        if cat == NlmCategory:\n",
    "            return string_element\n",
    "\n",
    "def get_segment(record, segment_label):\n",
    "    if record is None:\n",
    "        return None\n",
    "    return str(\n",
    "        get_attribute_text(\n",
    "            get(record, \"MedlineCitation.Article.Abstract.AbstractText\"), \n",
    "            segment_label\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ef3d84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import itertools\n",
    "\n",
    "#OLD one-off functions that need refactoring\n",
    "def sentence_has_phrase(sentence, cue_phrases):\n",
    "    for c in cue_phrases: #unsafe scope\n",
    "        if c in sentence:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "def find_sentence_in_abstract(paragraph, bias=0):\n",
    "    \"\"\"checks paragraph for key sentences. Returns first matching hit\"\"\"\n",
    "    p_sents = split_into_sentences(paragraph)\n",
    "    idx = find_index(p_sents, sentence_has_phrase)\n",
    "    if idx < 0:\n",
    "        return \"\"\n",
    "    \n",
    "    i = idx + bias #bias- for sentence before or after\n",
    "    if i < 0:\n",
    "        return \"\"\n",
    "    return get(p_sents, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b47964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "\n",
    "\n",
    "\n",
    "url = \"https://github.com/derekchuank/high-frequency-vocabulary/blob/master/30k.txt\"\n",
    "res = req.get(url)\n",
    "\n",
    "vocab30k = res.split(\"\\t\\n\")\n",
    "\n",
    "# file = open(“filename.txt”, “w”)\n",
    "# file.write(res.text)\n",
    "# file.close()\n",
    "vocab30k[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af8a9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_md\n",
    "with open(\"30k.txt\") as f:\n",
    "    vocab30k = f.read().split(\"\\t\\n\")\n",
    "\n",
    "nlp = en_core_web_md.load()\n",
    "\n",
    "extra_vocab = [\n",
    "               \"-PRON-\", \"-pron-\", \".\", \",\", \";\"  #lemmatizer inconsitency\n",
    "]\n",
    "\n",
    "special_tokens = {\n",
    "    \"(\": \"-lrb-\",\n",
    "    \")\": \"-rrb-\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3496dc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "def replace_outof_vocab_words(text, vocab=vocab30k, nlp=nlp, extra_vocab=extra_vocab, special_tokens=special_tokens):\n",
    "    vocab += extra_vocab\n",
    "    newtext = \"\"\n",
    "    for token in nlp(text):\n",
    "        t = token.text\n",
    "        if special_tokens.get(t):\n",
    "            t=special_tokens.get(t)\n",
    "        elif token.lemma_.lower() not in vocab and token.tag_ is not None:\n",
    "            t= \"ii\" + token.tag_.lower()\n",
    "        newtext += t + \" \"\n",
    "\n",
    "    return newtext\n",
    "    \n",
    "replace_outof_vocab_words('Pancreaticobiliary maljunction (PBM) refers to the union of the pancreatic and biliary ducts outside of the duodenal wall.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91b07ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLTK\n",
    "patterns = [\n",
    "    (r'.*ing$', 'VBG'),               # gerunds\n",
    "    (r'.*ed$', 'VBD'),                # simple past\n",
    "    (r'.*es$', 'VBZ'),                # 3rd singular present\n",
    "    (r'.*ould$', 'MD'),               # modals\n",
    "    (r'.*\\'s$', 'NN$'),               # possessive nouns\n",
    "    (r'.*s$', 'NNS'),                 # plural nouns\n",
    "    (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),  # cardinal numbers\n",
    "    (r'.*', 'NN')                     # nouns (default)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b76a0c",
   "metadata": {},
   "source": [
    "## Pubmed Abstract/Fulltext Fetching and Cleaning\n",
    "\n",
    "Pubmed does not give fulltext/abstract text in a good format. It gives a CSV with the metadata of searched results. That means you have to use the Entrez fetch and then merge the abstracts and text information with the metadata to have something workable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b7e4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"clinical_study_abst.txt\") as f:\n",
    "    text = f.read()\n",
    "    \n",
    "df = pd.read_csv(\"clin_study_meta.csv\")\n",
    "records = get_pubmed_records([str(pmid) for pmid in df[\"PMID\"]])\n",
    "len([r for r in records[\"PubmedArticle\"] if r is not None])\n",
    "\n",
    "articles = records[\"PubmedArticle\"]\n",
    "\n",
    "g = {\n",
    "    \"PMID\": [str(get(r,\"MedlineCitation.PMID\")) for r in articles],\n",
    "    \n",
    "    \"BACKGROUND\": [get_segment(r, \"BACKGROUND\") for r in articles],\n",
    "    \"OBJECTIVE\": [get_segment(r, \"OBJECTIVE\") for r in articles],\n",
    "    \"METHODS\": [get_segment(r, \"METHODS\") for r in articles],\n",
    "    \"RESULTS\": [get_segment(r, \"RESULTS\") for r in articles],\n",
    "    \"CONCLUSIONS\": [get_segment(r, \"CONCLUSIONS\") for r in articles]\n",
    "     }\n",
    "\n",
    "df2 = pd.DataFrame(g)\n",
    "df2[\"PMID\"] = df2[\"PMID\"].astype(int)\n",
    "dfx = pd.merge(df, df2, on=\"PMID\")\n",
    "dfx.to_csv(\"enriched_pm_10k.csv\")\n",
    "\n",
    "dfx.head(), len(df), len(df2), df[:6768][\"PMID\"][0], df2[:6768][\"PMID\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d7edb4",
   "metadata": {},
   "source": [
    "## Text Features with spacy extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba45dcf5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-43a7277e5b77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacytextblob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspacytextblob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSpacyTextBlob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_sm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spacytextblob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe('spacytextblob')\n",
    "text = 'I had a really horrible day. It was the worst day ever! But every now and then I have a really good day that makes me happy.'\n",
    "doc = nlp(text)\n",
    "doc._.polarity      # Polarity: -0.125\n",
    "doc._.subjectivity  # Sujectivity: 0.9\n",
    "doc._.assessments   # Assessments: [(['really', 'horrible'], -1.0, 1.0, None), (['worst', '!'], -1.0, 1.0, None), (['really', 'good'], 0.7, 0.6000000000000001, None), (['happy'], 0.8, 1.0, None)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baa5623",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
