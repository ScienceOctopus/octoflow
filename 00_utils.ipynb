{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f43257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641ba40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!sh setup.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "464ddf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6f5cb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting biopython\n",
      "  Downloading biopython-1.79-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.3 MB 29.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pydash\n",
      "  Downloading pydash-5.1.0-py3-none-any.whl (84 kB)\n",
      "\u001b[K     |████████████████████████████████| 84 kB 769 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages (from biopython) (1.19.5)\n",
      "Installing collected packages: pydash, biopython\n",
      "Successfully installed biopython-1.79 pydash-5.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install biopython pydash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da1f8154",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "from Bio import Entrez\n",
    "from pydash import get\n",
    "\n",
    "def get_pubmed_records(pmids):\n",
    "    Entrez.email = \"strasser.ms@gmail.com\"\n",
    "    # handle type is http.client.HTTPResponse\n",
    "    handle = Entrez.efetch(\n",
    "        db=\"pubmed\",\n",
    "        id=\",\".join(pmids),\n",
    "        #api_key=\"be67e0a1be023d17fff5334a6c6f45287a08\",\n",
    "        rettype=\"xml\",\n",
    "        retmode=\"text\",\n",
    "    )\n",
    "    records = Entrez.read(handle)\n",
    "    return records\n",
    "\n",
    "def get_attribute_text(AbstractText, NlmCategory):\n",
    "    if AbstractText is None:\n",
    "        return None\n",
    "    for string_element in AbstractText:\n",
    "        cat = \"\"\n",
    "        try:\n",
    "            #the worst API arbitrary nonsense ... 1h of my life for getting data out of it\n",
    "            cat = string_element.attributes[\"NlmCategory\"]\n",
    "        except:\n",
    "            None\n",
    "        \n",
    "        if cat == NlmCategory:\n",
    "            return string_element\n",
    "\n",
    "def get_segment(record, segment_label):\n",
    "    if record is None:\n",
    "        return None\n",
    "    return str(\n",
    "        get_attribute_text(\n",
    "            get(record, \"MedlineCitation.Article.Abstract.AbstractText\"), \n",
    "            segment_label\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "293346df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import itertools\n",
    "from pydash import find_index\n",
    "#OLD one-off functions that need refactoring\n",
    "def sentence_has_phrase(sentence, cue_phrases):\n",
    "    for c in cue_phrases: #unsafe scope\n",
    "        if c in sentence:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "def find_sentence_in_abstract(paragraph, bias=0):\n",
    "    \"\"\"checks paragraph for key sentences. Returns first matching hit\"\"\"\n",
    "    p_sents = split_into_sentences(paragraph)\n",
    "    idx = find_index(p_sents, sentence_has_phrase)\n",
    "    if idx < 0:\n",
    "        return \"\"\n",
    "    \n",
    "    i = idx + bias #bias- for sentence before or after\n",
    "    if i < 0:\n",
    "        return \"\"\n",
    "    return get(p_sents, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9b0af2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "\n",
    "# url = \"https://github.com/derekchuank/high-frequency-vocabulary/blob/master/30k.txt\"\n",
    "# res = req.get(url)\n",
    "# vocab30k = res.text.split(\"\\t\\n\")\n",
    "# vocab30k[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac5d4d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b59e89fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-69ade0be7f77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0men_core_web_md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vocab30k.txt\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "#export\n",
    "\n",
    "import spacy\n",
    "import en_core_web_md\n",
    "with open(\"vocab30k.txt\") as f:\n",
    "    vocab30k = f.read().split(\"\\t\\n\")\n",
    "\n",
    "nlp = en_core_web_md.load()\n",
    "\n",
    "extra_vocab = [\n",
    "               \"-PRON-\", \"-pron-\", \".\", \",\", \";\"  #lemmatizer inconsitency\n",
    "]\n",
    "\n",
    "special_tokens = {\n",
    "    \"(\": \"-lrb-\",\n",
    "    \")\": \"-rrb-\"\n",
    "}\n",
    "\n",
    "def replace_outof_vocab_words(text, vocab, nlp=nlp, extra_vocab=extra_vocab, special_tokens=special_tokens):\n",
    "    vocab += extra_vocab\n",
    "    newtext = \"\"\n",
    "    for token in nlp(text):\n",
    "        t = token.text\n",
    "        if special_tokens.get(t):\n",
    "            t=special_tokens.get(t)\n",
    "        elif token.lemma_.lower() not in vocab and token.tag_ is not None:\n",
    "            t= \"ii\" + token.tag_.lower()\n",
    "        newtext += t + \" \"\n",
    "\n",
    "    return newtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2624170",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace_outof_vocab_words('Pancreaticobiliary maljunction (PBM) refers to the union of the pancreatic and biliary ducts outside of the duodenal wall.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9d4de65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLTK\n",
    "patterns = [\n",
    "    (r'.*ing$', 'VBG'),               # gerunds\n",
    "    (r'.*ed$', 'VBD'),                # simple past\n",
    "    (r'.*es$', 'VBZ'),                # 3rd singular present\n",
    "    (r'.*ould$', 'MD'),               # modals\n",
    "    (r'.*\\'s$', 'NN$'),               # possessive nouns\n",
    "    (r'.*s$', 'NNS'),                 # plural nouns\n",
    "    (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),  # cardinal numbers\n",
    "    (r'.*', 'NN')                     # nouns (default)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb51ddf",
   "metadata": {},
   "source": [
    "## Pubmed Abstract/Fulltext Fetching and Cleaning\n",
    "\n",
    "Pubmed does not give fulltext/abstract text in a good format. It gives a CSV with the metadata of searched results. That means you have to use the Entrez fetch and then merge the abstracts and text information with the metadata to have something workable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2c1f1db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(       PMID                                              Title  \\\n",
       " 0  28946522  The effect of manual therapy and exercise in p...   \n",
       " 1  28325149  Low Dose Naltrexone in the Treatment of Fibrom...   \n",
       " 2  29514165  High-Dose Erythropoietin for Asphyxia and Ence...   \n",
       " 3  29424396  [Quality of sleep and academic performance in ...   \n",
       " 4  29754671  Rationale and design for the Vascular Outcomes...   \n",
       " \n",
       "                                              Authors  \\\n",
       " 0                 Ulger O, Demirel A, Oz M, Tamer S.   \n",
       " 1  Metyas S, Chen CL, Yeter K, Solyman J, Arkfeld...   \n",
       " 2  Juul SE, Comstock BA, Heagerty PJ, Mayock DE, ...   \n",
       " 3  Bugueño M, Curihual C, Olivares P, Wallace J, ...   \n",
       " 4  Capell WH, Bonaca MP, Nehler MR, Chen E, Kitte...   \n",
       " \n",
       "                                             Citation First Author  \\\n",
       " 0  J Back Musculoskelet Rehabil. 2017 Nov 6;30(6)...      Ulger O   \n",
       " 1  Curr Rheumatol Rev. 2018;14(2):177-180. doi: 1...     Metyas S   \n",
       " 2  Neonatology. 2018;113(4):331-338. doi: 10.1159...      Juul SE   \n",
       " 3  Rev Med Chil. 2017 Sep;145(9):1106-1114. doi: ...    Bugueño M   \n",
       " 4  Am Heart J. 2018 May;199:83-91. doi: 10.1016/j...    Capell WH   \n",
       " \n",
       "                    Journal/Book  Publication Year Create Date       PMCID  \\\n",
       " 0  J Back Musculoskelet Rehabil              2017  2017/09/27         NaN   \n",
       " 1            Curr Rheumatol Rev              2018  2017/03/23         NaN   \n",
       " 2                   Neonatology              2018  2018/03/08  PMC5980685   \n",
       " 3                  Rev Med Chil              2017  2018/02/10         NaN   \n",
       " 4                    Am Heart J              2018  2018/05/15         NaN   \n",
       " \n",
       "       NIHMS ID                                DOI  \\\n",
       " 0          NaN                 10.3233/BMR-169673   \n",
       " 1          NaN  10.2174/1573397113666170321120329   \n",
       " 2  NIHMS937494                  10.1159/000486820   \n",
       " 3          NaN    10.4067/s0034-98872017000901106   \n",
       " 4          NaN          10.1016/j.ahj.2018.01.011   \n",
       " \n",
       "                                           BACKGROUND  \\\n",
       " 0                                               None   \n",
       " 1  ConclusionFibromyalgia is a chronic pain disor...   \n",
       " 2                                               None   \n",
       " 3  Sleeping and studying are the day-to-day activ...   \n",
       " 4                                               None   \n",
       " \n",
       "                                          CONCLUSIONS  \\\n",
       " 0  This study showed that SSE and manual therapy ...   \n",
       " 1  This prospective study lends further support t...   \n",
       " 2                                               None   \n",
       " 3  A bad sleep quality influences academic perfor...   \n",
       " 4                                               None   \n",
       " \n",
       "                                              METHODS  \\\n",
       " 0  A total of one-hundred thirteen patients diagn...   \n",
       " 1  Naltrexone is an opioid receptor antagonist us...   \n",
       " 2                                               None   \n",
       " 3  Students of the first and second year of high ...   \n",
       " 4                                               None   \n",
       " \n",
       "                                            OBJECTIVE  \\\n",
       " 0  To determine the effects of spinal stabilizati...   \n",
       " 1  A significant number of fibromyalgia patients ...   \n",
       " 2                                               None   \n",
       " 3  To determine the quality of sleep and its rela...   \n",
       " 4                                               None   \n",
       " \n",
       "                                              RESULTS  \n",
       " 0  Intragroup analyses both treatments were effec...  \n",
       " 1  Two small prospective pilot studies have previ...  \n",
       " 2                                               None  \n",
       " 3  The interview was answered by 322 first year s...  \n",
       " 4                                               None  ,\n",
       " 10000,\n",
       " 10000,\n",
       " 28946522,\n",
       " 28946522)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with open(\"clinical_study_abst.txt\") as f:\n",
    "#     text = f.read()\n",
    "    \n",
    "df = pd.read_csv(\"downloads/clin_study_meta.csv\")\n",
    "records = get_pubmed_records([str(pmid) for pmid in df[\"PMID\"]])\n",
    "len([r for r in records[\"PubmedArticle\"] if r is not None])\n",
    "\n",
    "articles = records[\"PubmedArticle\"]\n",
    "\n",
    "g = {\n",
    "    \"PMID\": [str(get(r,\"MedlineCitation.PMID\")) for r in articles],\n",
    "    \"BACKGROUND\": [get_segment(r, \"BACKGROUND\") for r in articles],\n",
    "    \"OBJECTIVE\": [get_segment(r, \"OBJECTIVE\") for r in articles],\n",
    "    \"METHODS\": [get_segment(r, \"METHODS\") for r in articles],\n",
    "    \"RESULTS\": [get_segment(r, \"RESULTS\") for r in articles],\n",
    "    \"CONCLUSIONS\": [get_segment(r, \"CONCLUSIONS\") for r in articles]\n",
    "}\n",
    "\n",
    "\n",
    "df2 = pd.DataFrame(g)\n",
    "df2[\"PMID\"] = df2[\"PMID\"].astype(int)\n",
    "dfx = pd.merge(df, df2, on=\"PMID\")\n",
    "dfx.to_csv(\"enriched_pm_10k.csv\")\n",
    "\n",
    "\n",
    "dfx.head(), len(df), len(df2), df[:6768][\"PMID\"][0], df2[:6768][\"PMID\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9f84ec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_first(p):\n",
    "    return get(split_into_sentences(p), 0)\n",
    "#get(split_into_sentences(t), 0) for t in dfx[\"METHODS\"][:10] if split_into_sentences(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e25b0690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PMID</th>\n",
       "      <th>Title</th>\n",
       "      <th>source</th>\n",
       "      <th>DOI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [PMID, Title, source, DOI]\n",
       "Index: []"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "083f7527",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#reformating data\n",
    "dfy = pd.DataFrame([], columns=[\"PMID\", \"Title\", \"source\", \"DOI\", \"text\"])\n",
    "tags = [\"OBJECTIVE\", \"METHODS\", \"RESULTS\", \"CONCLUSIONS\"]\n",
    "for tag in tags:\n",
    "    df = dfx[dfx[tag] != \"None\"]\n",
    "    df[\"paragraph\"] = df[tag]\n",
    "    df[\"text\"] = dfx[tag].map(get_first)\n",
    "    df[\"source\"] = \"pm_\" + tag\n",
    "    df = df[dfy.columns]\n",
    "    dfy = dfy.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c4c5b59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfy = dfy.dropna(subset=[\"text\"])\n",
    "dfy[\"labels\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f111a6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfy.to_csv(\"downloads/struct_abstracts_negatives_18k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b9dfd5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A total of one-hundred thirteen patients diagnosed as CLBP were enrolled to the study.',\n",
       " 'Naltrexone is an opioid receptor antagonist used to treat alcohol and opioid dependence.',\n",
       " None,\n",
       " 'Students of the first and second year of high school answered an interview about socio-demographic background, academic performance, student activities and subjective sleep quality; they were evaluated using the Pittsburgh Sleep Quality Index (PSQI).',\n",
       " None,\n",
       " 'University students were randomly allocated into exercise (n: 28) and control (n: 25) groups.',\n",
       " 'A sample of 181 soon-to-graduate nursing students participated in the study and were assigned at random to one of two therapies: physical exercise (n = 90) or mindfulness meditation (n = 91).',\n",
       " None,\n",
       " 'A randomized study.',\n",
       " None]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[get(split_into_sentences(t), 0) for t in dfx[\"METHODS\"][:10] if split_into_sentences(t)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40974397",
   "metadata": {},
   "source": [
    "## Text Features with spacy extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389a8027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe('spacytextblob')\n",
    "text = 'I had a really horrible day. It was the worst day ever! But every now and then I have a really good day that makes me happy.'\n",
    "doc = nlp(text)\n",
    "doc._.polarity      # Polarity: -0.125\n",
    "doc._.subjectivity  # Sujectivity: 0.9\n",
    "doc._.assessments   # Assessments: [(['really', 'horrible'], -1.0, 1.0, None), (['worst', '!'], -1.0, 1.0, None), (['really', 'good'], 0.7, 0.6000000000000001, None), (['happy'], 0.8, 1.0, None)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97f1e75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
